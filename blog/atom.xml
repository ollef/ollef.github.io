<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Olle Fredriksson&#39;s blog</title>
    <link href="https://ollef.github.io/blog/atom.xml" rel="self" type="application/rss+xml" />
  <updated>2020-06-26T13:39:26Z</updated>
  <author>
      <name>Olle Fredriksson</name>
  </author>
  <id>https://ollef.github.io/blog/</id>

  <entry>
      <title>Query-based compiler architectures</title>
      <link href="https://ollef.github.io/blog/posts/query-based-compilers.html"/>
      <id>https://ollef.github.io/blog/posts/query-based-compilers.html</id>
      <updated>2020-06-25</updated>
      <summary></summary>
      <content type="html"><![CDATA[
          <img src="https://ollef.github.io/blog/images/query-based-compilers/top.png" alt="Query-based compiler architectures">
              <p>Note: This is an old post originally from the documentation of the <a href="https://github.com/ollef/sixten">Sixten</a> programming language, that I've touched up and fleshed out. After the time that it was written I've found out about <a href="https://github.com/salsa-rs/salsa">Salsa</a>, a Rust library with very similar goals to my Rock library, which is definitely worth checking out as well!</p>
<h2 id="background">Background</h2>
<p>Compilers are no longer just black boxes that take a bunch of source files and produce assembly code. We expect them to:</p>
<ul>
<li>Be incremental, meaning that if we recompile a project after having made a few changes we only recompile what is affected by the changes.</li>
<li>Provide editor tooling, e.g. through a <a href="https://langserver.org/">language server</a>, supporting functionality like going to definition, finding the type of the expression at a specific location, and showing error messages on the fly.</li>
</ul>
<p>This is what Anders Hejlsberg talks about in <a href="https://www.youtube.com/watch?v=wSdV1M7n4gQ">his video on modern compiler construction</a> that some of you might have seen.</p>
<p>In this post I will cover how this is achieved in <a href="https://github.com/ollef/sixten">Sixten</a> by building the compiler around a query system.</p>
<p>For those of you that don't know, Sixten is an experimental functional programming language created to give the programmer more control over memory layout and boxing than most other high-level languages do. The most recent development of Sixten is being done in the <a href="https://github.com/ollef/sixty">Sixty</a> repository, and is completely query-based. Here's a little video giving a taste of what its language server can do, showing type-based completions:</p>
<script id="asciicast-V7rsch6mLtFPTrWmlCTAMDzcn" src="https://asciinema.org/a/V7rsch6mLtFPTrWmlCTAMDzcn.js" data-rows="12" async></script>
<h2 id="traditional-pipeline-based-compiler-architectures">Traditional pipeline-based compiler architectures</h2>
<p>A traditional compiler pipeline might look a bit like this:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode md"><code class="sourceCode markdown"><span id="cb1-1"><a href="#cb1-1"></a>+-----------+            +-----+                +--------+               +--------+</span>
<span id="cb1-2"><a href="#cb1-2"></a>|           |            |     |                |        |               |        |</span>
<span id="cb1-3"><a href="#cb1-3"></a>|source text|---parse---&gt;| AST |---typecheck-+-&gt;|core AST|---generate---&gt;|assembly|</span>
<span id="cb1-4"><a href="#cb1-4"></a>|           |            |     |       ^        |        |               |        |</span>
<span id="cb1-5"><a href="#cb1-5"></a>+-----------+            +-----+       |        +--------+               +---------</span>
<span id="cb1-6"><a href="#cb1-6"></a>                                       |</span>
<span id="cb1-7"><a href="#cb1-7"></a>                                 read and write</span>
<span id="cb1-8"><a href="#cb1-8"></a>                                     types</span>
<span id="cb1-9"><a href="#cb1-9"></a>                                       |</span>
<span id="cb1-10"><a href="#cb1-10"></a>                                       v</span>
<span id="cb1-11"><a href="#cb1-11"></a>                                  +----------+</span>
<span id="cb1-12"><a href="#cb1-12"></a>                                  |          |</span>
<span id="cb1-13"><a href="#cb1-13"></a>                                  |type table|</span>
<span id="cb1-14"><a href="#cb1-14"></a>                                  |          |</span>
<span id="cb1-15"><a href="#cb1-15"></a>                                  +----------+</span></code></pre></div>
<p>There are many variations, and often more steps and intermediate representations than in the illustration, but the idea stays the same:</p>
<p>We push source text down a pipeline and run a fixed set of transformations until we finally output assembly code or some other target language. Along the way we often need to read and update some state. For example, we might update a type table during type checking so we can later look up the type of entities that the code refers to.</p>
<p>Traditional compiler pipelines are probably quite familiar to many of us, but how query-based compilers should be architected might not be as well-known. Here I will describe one way to do it.</p>
<h2 id="going-from-pipeline-to-queries">Going from pipeline to queries</h2>
<p>What does it take to get the type of a qualified name, such as <code>"Data.List.map"</code>? In a pipeline-based architecture we would just look it up in the type table. With queries, we have to think differently. Instead of relying on having updated some piece of state, we do it as if it was done from scratch.</p>
<p>As a first iteration, we do it <em>completely</em> from scratch. It might look a little bit like this:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb2-1"><a href="#cb2-1"></a><span class="ot">fetchType ::</span> <span class="dt">QualifiedName</span> <span class="ot">-&gt;</span> <span class="dt">IO</span> <span class="dt">Type</span></span>
<span id="cb2-2"><a href="#cb2-2"></a>fetchType (<span class="dt">QualifiedName</span> moduleName name) <span class="ot">=</span> <span class="kw">do</span></span>
<span id="cb2-3"><a href="#cb2-3"></a>  fileName <span class="ot">&lt;-</span> moduleFileName moduleName</span>
<span id="cb2-4"><a href="#cb2-4"></a>  sourceCode <span class="ot">&lt;-</span> <span class="fu">readFile</span> fileName</span>
<span id="cb2-5"><a href="#cb2-5"></a>  parsedModule <span class="ot">&lt;-</span> parseModule sourceCode</span>
<span id="cb2-6"><a href="#cb2-6"></a>  resolvedModule <span class="ot">&lt;-</span> resolveNames parsedModule</span>
<span id="cb2-7"><a href="#cb2-7"></a>  <span class="kw">let</span> definition <span class="ot">=</span> <span class="fu">lookup</span> name resolvedModule</span>
<span id="cb2-8"><a href="#cb2-8"></a>  inferDefinitionType definition</span></code></pre></div>
<p>We first find out what file the name comes from, which might be <code>Data/List.vix</code> for <code>Data.List</code>, then read the contents of the file, parse it, perhaps we do name resolution to find out what the names in the code refer to given what is imported, and last we look up the name-resolved definition and type check it, returning its type.</p>
<p>All this for just for getting the type of an identifier? It seems ridiculous because looking up the type of a name is something we'll do loads of times during the type checking of a module. Luckily we're not done yet.</p>
<p>Let's first refactor the code into smaller functions:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb3-1"><a href="#cb3-1"></a><span class="ot">fetchParsedModule ::</span> <span class="dt">ModuleName</span> <span class="ot">-&gt;</span> <span class="dt">IO</span> <span class="dt">ParsedModule</span></span>
<span id="cb3-2"><a href="#cb3-2"></a>fetchParsedModule moduleName <span class="ot">=</span> <span class="kw">do</span></span>
<span id="cb3-3"><a href="#cb3-3"></a>  fileName <span class="ot">&lt;-</span> moduleFileName moduleName</span>
<span id="cb3-4"><a href="#cb3-4"></a>  sourceCode <span class="ot">&lt;-</span> <span class="fu">readFile</span> fileName</span>
<span id="cb3-5"><a href="#cb3-5"></a>  parseModule moduleName</span>
<span id="cb3-6"><a href="#cb3-6"></a></span>
<span id="cb3-7"><a href="#cb3-7"></a><span class="ot">fetchResolvedModule ::</span> <span class="dt">ModuleName</span> <span class="ot">-&gt;</span> <span class="dt">IO</span> <span class="dt">ResolvedModule</span></span>
<span id="cb3-8"><a href="#cb3-8"></a>fetchResolvedModule moduleName <span class="ot">=</span> <span class="kw">do</span></span>
<span id="cb3-9"><a href="#cb3-9"></a>  parsedModule <span class="ot">&lt;-</span> fetchParsedModule moduleName</span>
<span id="cb3-10"><a href="#cb3-10"></a>  resolveNames parsedModule</span>
<span id="cb3-11"><a href="#cb3-11"></a></span>
<span id="cb3-12"><a href="#cb3-12"></a><span class="ot">fetchType ::</span> <span class="dt">QualifiedName</span> <span class="ot">-&gt;</span> <span class="dt">IO</span> <span class="dt">Type</span></span>
<span id="cb3-13"><a href="#cb3-13"></a>fetchType (<span class="dt">QualifiedName</span> moduleName name) <span class="ot">=</span> <span class="kw">do</span></span>
<span id="cb3-14"><a href="#cb3-14"></a>  resolvedModule <span class="ot">&lt;-</span> fetchResolvedModule moduleName</span>
<span id="cb3-15"><a href="#cb3-15"></a>  <span class="kw">let</span> definition <span class="ot">=</span> <span class="fu">lookup</span> name resolvedModule</span>
<span id="cb3-16"><a href="#cb3-16"></a>  inferDefinitionType definition</span></code></pre></div>
<p>Note that each of the functions do everything from scratch on their own, i.e. they're each doing a (longer and longer) prefix of the work you'd do in a pipeline. I've found this to be a common pattern in my query-based compilers.</p>
<p>One way to make this efficient would be to add a memoisation layer around each function. That way, we do some expensive work the first time we invoke a function with a specific argument, but subsequent calls are cheap as they can return the cached result.</p>
<p>This is essentially what we'll do, but we won't use a separate cache per function, but instead have a central cache, indexed by the query. This functionality is provided by <a href="https://github.com/ollef/rock">Rock</a>, a library that packages up some functionality for creating query-based compilers.</p>
<h2 id="the-rock-library">The Rock library</h2>
<p><a href="https://github.com/ollef/rock">Rock</a> is an experimental library heavily inspired by <a href="https://github.com/ndmitchell/shake">Shake</a> and the <a href="https://www.microsoft.com/en-us/research/publication/build-systems-la-carte/">Build systems à la carte paper</a>. It essentially implements a build system framework, like <code>make</code>.</p>
<p>Build systems have a lot in common with modern compilers since we want them to be incremental, i.e. to take advantage of previous build results when building anew with few changes. But there's also a difference: Most build systems don't care about the <em>types</em> of their queries since they work at the level of files and file systems.</p>
<p><em>Build systems à la carte</em> is closer to what we want. There the user writes a bunch of computations, <em>tasks</em>, choosing a suitable type for keys and a type for values. The tasks are formulated assuming they're run in an environment where there is a function <code>fetch</code> of type <code>Key -&gt; Task Value</code>, where <code>Task</code> is a type for describing build system rules, that can be used to fetch the value of a dependency with a specific key. In our above example, the key type might look like this:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb4-1"><a href="#cb4-1"></a><span class="kw">data</span> <span class="dt">Key</span></span>
<span id="cb4-2"><a href="#cb4-2"></a>  <span class="ot">=</span> <span class="dt">ParsedModuleKey</span> <span class="dt">ModuleName</span></span>
<span id="cb4-3"><a href="#cb4-3"></a>  <span class="op">|</span> <span class="dt">ResolvedModuleKey</span> <span class="dt">ModuleName</span></span>
<span id="cb4-4"><a href="#cb4-4"></a>  <span class="op">|</span> <span class="dt">TypeKey</span> <span class="dt">QualifiedName</span></span></code></pre></div>
<p>The build system has control over what code runs when we do a <code>fetch</code>, so by varying that it can do fine-grained dependency tracking, memoisation, and incremental updates.</p>
<p><em>Build systems à la carte</em> is also about exploring what kind of build systems we get when we vary what <code>Task</code> is allowed to do, e.g. if it's a <code>Monad</code> or <code>Applicative</code>. In Rock, we're not exploring <em>that</em>, so our <code>Task</code> is a thin layer on top of <code>IO</code>.</p>
<p>A problem that pops up now, however, is that there's no satisfactory type for <code>Value</code>. We want <code>fetch (ParsedModuleKey "Data.List")</code> to return a <code>ParsedModule</code>, while <code>fetch (TypeKey "Data.List.map")</code> should return something of type <code>Type</code>.</p>
<h3 id="indexed-queries">Indexed queries</h3>
<p>Rock allows us to index the key type by the return type of the query. The <code>Key</code> type in our running example becomes the following <a href="https://en.wikipedia.org/wiki/Generalized_algebraic_data_type">GADT</a>:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb5-1"><a href="#cb5-1"></a><span class="kw">data</span> <span class="dt">Key</span> a <span class="kw">where</span></span>
<span id="cb5-2"><a href="#cb5-2"></a>  <span class="dt">ParsedModuleKey</span><span class="ot"> ::</span> <span class="dt">ModuleName</span> <span class="ot">-&gt;</span> <span class="dt">Key</span> <span class="dt">ParsedModule</span></span>
<span id="cb5-3"><a href="#cb5-3"></a>  <span class="dt">ResolvedModuleKey</span><span class="ot"> ::</span> <span class="dt">ModuleName</span> <span class="ot">-&gt;</span> <span class="dt">Key</span> <span class="dt">ResolvedModule</span></span>
<span id="cb5-4"><a href="#cb5-4"></a>  <span class="dt">TypeKey</span><span class="ot"> ::</span> <span class="dt">QualifiedName</span> <span class="ot">-&gt;</span> <span class="dt">Key</span> <span class="dt">Type</span></span></code></pre></div>
<p>The <code>fetch</code> function gets the type <code>forall a. Key a -&gt; Task a</code>, so we get a <code>ParsedModule</code> when we run <code>fetch (ParsedModuleKey "Data.List")</code>, like we wanted, because the return type depends on the key we use.</p>
<p>Now that we know what <code>fetch</code> should look like, it's also worth revealing what the <code>Task</code> type looks like in Rock, more concretely. As mentioned, it's a thin layer around <code>IO</code>, providing a way to <code>fetch</code> <code>key</code>s (like <code>Key</code> above):</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb6-1"><a href="#cb6-1"></a><span class="kw">newtype</span> <span class="dt">Task</span> key a <span class="ot">=</span> <span class="dt">Task</span> {<span class="ot"> unTask ::</span> <span class="dt">ReaderT</span> (<span class="dt">Fetch</span> key) <span class="dt">IO</span> a }</span>
<span id="cb6-2"><a href="#cb6-2"></a><span class="kw">newtype</span> <span class="dt">Fetch</span> key <span class="ot">=</span> <span class="dt">Fetch</span> (<span class="kw">forall</span> a<span class="op">.</span> key a <span class="ot">-&gt;</span> <span class="dt">IO</span> a)</span></code></pre></div>
<p>The rules of our compiler, i.e. its "Makefile", then becomes the following function, reusing the functions from above:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb7-1"><a href="#cb7-1"></a><span class="ot">rules ::</span> <span class="dt">Key</span> a <span class="ot">-&gt;</span> <span class="dt">Task</span> a</span>
<span id="cb7-2"><a href="#cb7-2"></a>rules key <span class="ot">=</span> <span class="kw">case</span> key <span class="kw">of</span></span>
<span id="cb7-3"><a href="#cb7-3"></a>  <span class="dt">ParsedModuleKey</span> moduleName <span class="ot">-&gt;</span></span>
<span id="cb7-4"><a href="#cb7-4"></a>    fetchParsedModule moduleName</span>
<span id="cb7-5"><a href="#cb7-5"></a></span>
<span id="cb7-6"><a href="#cb7-6"></a>  <span class="dt">ResolvedModuleKey</span> moduleName <span class="ot">-&gt;</span></span>
<span id="cb7-7"><a href="#cb7-7"></a>    fetchResolvedModule moduleName</span>
<span id="cb7-8"><a href="#cb7-8"></a></span>
<span id="cb7-9"><a href="#cb7-9"></a>  <span class="dt">TypeKey</span> qualifiedName <span class="ot">-&gt;</span></span>
<span id="cb7-10"><a href="#cb7-10"></a>    fetchType qualifiedName</span></code></pre></div>
<h3 id="caching">Caching</h3>
<p>The most basic way to run a <code>Task</code> in Rock is to directly call the <code>rules</code> function when a <code>Task</code> fetches a key. This results in an inefficient build system that recomputes every query from scratch.</p>
<p>But the <code>Rock</code> library lets us layer more functionality onto our <code>rules</code> function, and one thing that we can add is memoisation. If we do that Rock caches the result of each fetched key by storing the key-value pairs of already performed fetches in a <a href="https://hackage.haskell.org/package/dependent-hashmap">dependent hashmap</a>. This way, we perform each query at most once during a single run of the compiler.</p>
<h3 id="verifying-dependencies-and-reusing-state">Verifying dependencies and reusing state</h3>
<p>Another kind of functionality that can be layered onto the <code>rules</code> function is incremental updates. When it's used, Rock keeps a track of what dependencies a task used when it was executed (much like Shake) in a table, i.e. what keys it fetched and what the values were. Using this information it's able to determine when it's safe to reuse the cache <em>from a previous run of the compiler</em> even though there might be changes in other parts of the dependency graph.</p>
<p>This fine-grained dependency tracking also allows reusing the cache when a dependency of a task changes in a way that has no effect. For example, whitespace changes might trigger a re-parse, but since the AST is the same, the cache can be reused in queries that depend on the parse result.</p>
<h3 id="reverse-dependency-tracking">Reverse dependency tracking</h3>
<p>Verifying dependencies can be too slow for real-time tooling like language servers, because large parts of the dependency graph have to be traversed just to check that most of it is unchanged even for tiny changes.</p>
<p>For example, if we make changes to a source file with many large imports, we need to walk the dependency trees of all of the imports just to update the editor state for that single file. This is because dependency verification by itself needs to walk all the way to the root queries for all the dependencies of a given query, which can often be a large proportion of the whole dependency tree.</p>
<p>To fix this, Rock can also be made to track <em>reverse</em> dependencies between queries. When e.g. a language server detects that a single file has changed, the reverse dependency tree is used invalidate the cache just for the queries that depend on that file by walking the reverse dependencies starting from the changed file.</p>
<p>Since the imported modules don't depend on that file, they don't need re-checked, resulting in much snappier tooling!</p>
<h2 id="closing-thoughts">Closing thoughts</h2>
<p>Most modern languages need to have a strategy for tooling, and building compilers around query systems seems like an extremely promising approach to me.</p>
<p>With queries the compiler writer doesn't have to handle updates to and invalidation of a bunch of ad-hoc caches, which can be the result when adding incremental updates to a traditional compiler pipeline. In a query-based system it's all handled centrally once and for all, which means there's less of a chance it's wrong.</p>
<p>Queries are excellent for tooling because they allow us to ask for the value of any query at any time without worrying about order or temporal effects, just like a well-written Makefile. The system will compute or retrieve cached values for the query and its dependencies automatically in an incremental way.</p>
<p>Query-based compilers are also surprisingly easy to parallelise. Since we're allowed to make any query at any time, and they're memoised the first time they're run, we can fire off queries in parallel without having to think much. In Sixty, the default behaviour is for all input modules to be type checked in parallel.</p>
<p>Lastly, I hope that this post will have inspired you to use a query-based compiler architecture, and given you an idea of how it can be done.</p>
      ]]></content>
      </entry>
  <entry>
      <title>Speeding up the Sixty compiler</title>
      <link href="https://ollef.github.io/blog/posts/speeding-up-sixty.html"/>
      <id>https://ollef.github.io/blog/posts/speeding-up-sixty.html</id>
      <updated>2020-04-23</updated>
      <summary></summary>
      <content type="html"><![CDATA[
          <img src="https://ollef.github.io/blog/images/speeding-up-sixty/9-d5bad6f606450d0a2c8926072e7b4845d982b81f-threadscope.png" alt="Speeding up the Sixty compiler">
              <h2 id="background">Background</h2>
<p>I'm working on a reimplementation of <a href="https://github.com/ollef/sixten">Sixten</a>, a dependently typed programming language that supports unboxed data. The reimplementation currently lives in a separate repository, and is called <a href="https://github.com/ollef/sixty">Sixty</a>, though the intention is that it going to replace Sixten eventually. The main reason for doing a reimplementation is to try out some implementation techniques to make the type checker faster, inspired by András Kovács' <a href="https://github.com/AndrasKovacs/smalltt">smalltt</a>.</p>
<p>In this post I'm going to show some optimisations that I implemented recently. I will also show the workflow and profiling tools that I use to find <em>what</em> to optimise in Haskell programs such as Sixty.</p>
<h2 id="a-benchmark">A benchmark</h2>
<p>What set me off was that I was curious to see how Sixty would handle programs with many modules. The problem was that no one had ever written any large programs in the language so far.</p>
<p>As a substitute, I added a command to generate nonsense programs of a given size. The programs that I used for the benchmarks in this post consist of just over 10 000 lines divided into 100 modules that all look like this:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb1-1"><a href="#cb1-1"></a><span class="kw">module</span> <span class="dt">Module60</span> exposing (<span class="op">..</span>)</span>
<span id="cb1-2"><a href="#cb1-2"></a></span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="kw">import</span> <span class="dt">Module9</span></span>
<span id="cb1-4"><a href="#cb1-4"></a><span class="kw">import</span> <span class="dt">Module24</span></span>
<span id="cb1-5"><a href="#cb1-5"></a><span class="kw">import</span> <span class="dt">Module35</span></span>
<span id="cb1-6"><a href="#cb1-6"></a><span class="kw">import</span> <span class="dt">Module16</span></span>
<span id="cb1-7"><a href="#cb1-7"></a><span class="kw">import</span> <span class="dt">Module46</span></span>
<span id="cb1-8"><a href="#cb1-8"></a><span class="kw">import</span> <span class="dt">Module37</span></span>
<span id="cb1-9"><a href="#cb1-9"></a><span class="kw">import</span> <span class="dt">Module50</span></span>
<span id="cb1-10"><a href="#cb1-10"></a><span class="kw">import</span> <span class="dt">Module47</span></span>
<span id="cb1-11"><a href="#cb1-11"></a><span class="kw">import</span> <span class="dt">Module46</span></span>
<span id="cb1-12"><a href="#cb1-12"></a><span class="kw">import</span> <span class="dt">Module3</span></span>
<span id="cb1-13"><a href="#cb1-13"></a></span>
<span id="cb1-14"><a href="#cb1-14"></a>f1 <span class="op">:</span> <span class="dt">Type</span></span>
<span id="cb1-15"><a href="#cb1-15"></a>f1 <span class="ot">=</span> Module46.f10 <span class="ot">-&gt;</span> Module46.f20</span>
<span id="cb1-16"><a href="#cb1-16"></a></span>
<span id="cb1-17"><a href="#cb1-17"></a>f2 <span class="op">:</span> <span class="dt">Type</span></span>
<span id="cb1-18"><a href="#cb1-18"></a>f2 <span class="ot">=</span> Module50.f24 <span class="ot">-&gt;</span> Module47.f13</span>
<span id="cb1-19"><a href="#cb1-19"></a></span>
<span id="cb1-20"><a href="#cb1-20"></a>[<span class="op">...</span>]</span>
<span id="cb1-21"><a href="#cb1-21"></a></span>
<span id="cb1-22"><a href="#cb1-22"></a>f30 <span class="op">:</span> <span class="dt">Type</span></span>
<span id="cb1-23"><a href="#cb1-23"></a>f30 <span class="ot">=</span> Module37.f4 <span class="ot">-&gt;</span> Module24.f24</span></code></pre></div>
<p>Each module is about 100 lines of code, of which a third or so are newlines, and has thirty definitions that refer to definitions from other modules. The definitions are simple enough to be type checked very quickly, so the benchmark makes us focus mostly on other parts of the compiler.</p>
<p>I'd also like to write about the type checker itself, but will save that for other posts.</p>
<h2 id="profiling">Profiling</h2>
<p>I use three main tools to try to identify bottlenecks and other things to improve:</p>
<ul>
<li><p><a href="http://www.haskellforall.com/2016/05/a-command-line-benchmark-tool.html">bench</a> is a replacement for the Unix <code>time</code> command that I use to get more reliable timings, especially useful for comparing the before and after time of some change.</p></li>
<li><p>GHC's built-in profiling support, which gives us a detailed breakdown of where time is spent when running the program.</p>
<p>When using Stack, we can build with profiling by issuing:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb2-1"><a href="#cb2-1"></a><span class="ex">stack</span> install --profile</span></code></pre></div>
<p>Then we can run the program with profiling enabled:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb3-1"><a href="#cb3-1"></a><span class="ex">sixty</span> check +RTS -p</span></code></pre></div>
<p>This produces a file <code>sixty.prof</code> that contains the profiling information.</p>
<p>I also really like to use <a href="https://github.com/fpco/ghc-prof-flamegraph">ghc-prof-flamegraph</a> to turn the profiling output into a flamegraph:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb4-1"><a href="#cb4-1"></a><span class="ex">ghc-prof-flamegraph</span> sixty.prof</span></code></pre></div></li>
<li><p><a href="https://wiki.haskell.org/ThreadScope">Threadscope</a> is a visual tool for debugging the parallelism in a Haskell program. It also shows when the garbage collector runs, so can be used when tuning garbage collector parameters.</p></li>
</ul>
<h2 id="baseline-and-initial-profiling">Baseline and initial profiling</h2>
<p>We start out on <a href="https://github.com/ollef/sixty/tree/29094e006d4c88f51d744b0fd26f3e2e18af3ce0">this commit</a>.</p>
<p>Running <code>sixty check</code> on the 100 module project on my machine gives us our baseline:</p>
<table>
<thead>
<tr class="header">
<th></th>
<th style="text-align: right;">Time</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Baseline</td>
<td style="text-align: right;">1.30 s</td>
</tr>
</tbody>
</table>
<p>The flamegraph of the profiling output looks like this:</p>
<p><a href="../images/speeding-up-sixty/0-29094e006d4c88f51d744b0fd26f3e2e18af3ce0.svg"><img src="../images/speeding-up-sixty/0-29094e006d4c88f51d744b0fd26f3e2e18af3ce0.svg" /></a></p>
<p>Two things stick out to me in the flamegraph:</p>
<ul>
<li>Parsing takes about 45 % of the time.</li>
<li>Operations on <a href="https://hackage.haskell.org/package/dependent-map"><code>Data.Dependent.Map</code></a> take about 15 % of the time, and a large part of that is calls to <code>Query.gcompare</code> when the map is doing key comparisons during lookups and insertions.</li>
</ul>
<p>Here's what a run looks like in ThreadScope:</p>
<p><a href="../images/speeding-up-sixty/0-29094e006d4c88f51d744b0fd26f3e2e18af3ce0-threadscope.png"><img src="../images/speeding-up-sixty/0-29094e006d4c88f51d744b0fd26f3e2e18af3ce0-threadscope.png" /></a></p>
<p>And here's a more zoomed in ThreadScope picture:</p>
<p><a href="../images/speeding-up-sixty/0-29094e006d4c88f51d744b0fd26f3e2e18af3ce0-threadscope-detail.png"><img src="../images/speeding-up-sixty/0-29094e006d4c88f51d744b0fd26f3e2e18af3ce0-threadscope-detail.png" /></a></p>
<p>I note the following in the ThreadScope output:</p>
<ul>
<li>One core is doing almost all of the work, with other cores only occasionally performing very short tasks.</li>
<li>Garbage collection runs extremely often and takes just over 20 % of the time.</li>
</ul>
<h2 id="optimisation-1-better-rts-flags">Optimisation 1: Better RTS flags</h2>
<p>As we saw in the ThreadScope output, garbage collection runs often and takes a large part of the total runtime of the type checker.</p>
<p>In <a href="https://github.com/ollef/sixty/tree/f8d4ee7ee0d3d617c6d30401592f5639be60b14a">this commit</a> I most notably introduce the RTS option <code>-A50m</code>, which sets the default allocation area size used by the garbage collector to 50 MB, instead of the default 1 MB, which means that GC can run less often, potentially at the cost of worse cache behaviour and memory use. The value <code>50m</code> was found to be the sweet spot for performance on my machine by trying some different values.</p>
<p>The result of this change is this:</p>
<table>
<thead>
<tr class="header">
<th></th>
<th style="text-align: right;">Time</th>
<th style="text-align: right;">Delta</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Baseline</td>
<td style="text-align: right;">1.30 s</td>
<td style="text-align: right;"></td>
</tr>
<tr class="even">
<td>RTS flags</td>
<td style="text-align: right;">1.08 s</td>
<td style="text-align: right;">-17 %</td>
</tr>
</tbody>
</table>
<p>The ThreadScope output shows that the change has a very noticeable effect of decreasing the number of garbage collections:</p>
<p><a href="../images/speeding-up-sixty/1-f8d4ee7ee0d3d617c6d30401592f5639be60b14a-threadscope.png"><img src="../images/speeding-up-sixty/1-f8d4ee7ee0d3d617c6d30401592f5639be60b14a-threadscope.png" /></a></p>
<p>Also note that the proportion of time used by the GC went from 20 % to 3 %, which seems good to me.</p>
<h2 id="optimisation-2-a-couple-of-rock-library-improvements">Optimisation 2: A couple of Rock library improvements</h2>
<p><a href="https://github.com/ollef/rock">Rock</a> is a library that's used to implement query-based compilation in Sixty. I made two improvements to it that made Sixty almost twice as fast at the task:</p>
<table>
<thead>
<tr class="header">
<th></th>
<th style="text-align: right;">Time</th>
<th style="text-align: right;">Delta</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Baseline</td>
<td style="text-align: right;">1.30 s</td>
<td style="text-align: right;"></td>
</tr>
<tr class="even">
<td>RTS flags</td>
<td style="text-align: right;">1.08 s</td>
<td style="text-align: right;">-17 %</td>
</tr>
<tr class="odd">
<td>Rock</td>
<td style="text-align: right;">0.613 s</td>
<td style="text-align: right;">-43 %</td>
</tr>
</tbody>
</table>
<p>The changes are:</p>
<ul>
<li>Using <code>IORef</code>s and atomic operations instead of <code>MVar</code>s: Rock uses a cache e.g. to keep track of what queries have already been executed. This cache is potentially accessed and updated from different threads. Before this change this state was stored in an <code>MVar</code>, but since it's only doing fairly simple updates, the atomic operations of <code>IORef</code> are sufficient.</li>
<li>Being a bit more clever about the automatic parallelisation: We'll get back to this, but at this point in time Rock uses a <a href="https://github.com/facebook/Haxl">Haxl</a>-like automatic parallelisation scheme, running queries done in an <code>Applicative</code> context in parallel. The change here is to only trigger parallel query execution if both sides of an application of the <code>&lt;*&gt;</code> operator do queries that are not already cached. Before this change even the cache lookup part of the queries was done in parallel, which is likely far too fine-grained to pay off.</li>
</ul>
<p>We can clearly see in ThreadScope that the parallelisation has a seemingly good effect for part of the runtime, but not all of it:</p>
<p><a href="../images/speeding-up-sixty/2-54b87689f345173dbed3510a396641cd8c5e43f2-threadscope.png"><img src="../images/speeding-up-sixty/2-54b87689f345173dbed3510a396641cd8c5e43f2-threadscope.png" /></a></p>
<p>Unfortunately I didn't update Sixty in between the two changes, so I don't really know how much each one contributes.</p>
<h2 id="optimisation-3-manual-query-parallelisation">Optimisation 3: Manual query parallelisation</h2>
<p>I wasn't quite happy with the automatic parallelism since it mostly resulted in sequential execution. To improve on that, I removed the automatic parallelism support from the Rock library, and started doing it manually instead.</p>
<p>Code wise <a href="https://github.com/ollef/sixty/commit/7ca773e347dae952d4c7249a0310f10077a2474b">this change is quite small</a>. It's going from</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb5-1"><a href="#cb5-1"></a>checkAll <span class="ot">=</span> <span class="kw">do</span></span>
<span id="cb5-2"><a href="#cb5-2"></a>  inputFiles <span class="ot">&lt;-</span> fetch <span class="dt">Query.InputFiles</span></span>
<span id="cb5-3"><a href="#cb5-3"></a>  forM_ inputFiles checkFile</span></code></pre></div>
<p>to</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb6-1"><a href="#cb6-1"></a>checkAll <span class="ot">=</span> <span class="kw">do</span></span>
<span id="cb6-2"><a href="#cb6-2"></a>  inputFiles <span class="ot">&lt;-</span> fetch <span class="dt">Query.InputFiles</span></span>
<span id="cb6-3"><a href="#cb6-3"></a>  pooledForConcurrently_ inputFiles checkFile</span></code></pre></div>
<p>where <code>pooledForConcurrently_</code> is a variant of <code>forM_</code> that runs in parallel, using pooling to keep the number of threads the same as the number of cores on the machine it's run on.</p>
<p>Here are the timings:</p>
<table>
<thead>
<tr class="header">
<th></th>
<th style="text-align: right;">Time</th>
<th style="text-align: right;">Delta</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Baseline</td>
<td style="text-align: right;">1.30 s</td>
<td style="text-align: right;"></td>
</tr>
<tr class="even">
<td>RTS flags</td>
<td style="text-align: right;">1.08 s</td>
<td style="text-align: right;">-17 %</td>
</tr>
<tr class="odd">
<td>Rock</td>
<td style="text-align: right;">0.613 s</td>
<td style="text-align: right;">-43 %</td>
</tr>
<tr class="even">
<td>Manual parallelisation</td>
<td style="text-align: right;">0.451 s</td>
<td style="text-align: right;">-26 %</td>
</tr>
</tbody>
</table>
<p>Being able to type check modules in parallel on a whim like this seems to be a great advantage of using a query-based architecture. The modules can be processed in any order, and any non-processed dependencies that are missing are processed and cached on an as-needed basis.</p>
<p>ThreadScope shows that the CPU core utilisation is improved, even though the timings aren't as much better as one might expect from the image:</p>
<p><a href="../images/speeding-up-sixty/4-7ca773e347dae952d4c7249a0310f10077a2474b-threadscope.png"><img src="../images/speeding-up-sixty/4-7ca773e347dae952d4c7249a0310f10077a2474b-threadscope.png" /></a></p>
<p>The flamegraph is also interesting, because the proportion of time that goes to parsing has gone down to about 17 % without having made any changes to the parser, which can be seen in the top-right part of the image:</p>
<p><a href="../images/speeding-up-sixty/4-7ca773e347dae952d4c7249a0310f10077a2474b.svg"><img src="../images/speeding-up-sixty/4-7ca773e347dae952d4c7249a0310f10077a2474b.svg" /></a></p>
<p>This might indicate that that part of the compiler parallelises well.</p>
<h2 id="optimisation-4-parser-lookahead">Optimisation 4: Parser lookahead</h2>
<p>Here's an experiment that only helped a little. As we just saw, parsing still takes quite a large proportion of the total time spent, almost 17 %, so I wanted to make it faster.</p>
<p>The parser is written using parsing combinators, and the "inner loop" of e.g. the term parser is a choice between a bunch of different alternatives. Something like this:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb7-1"><a href="#cb7-1"></a><span class="ot">term ::</span> <span class="dt">Parser</span> <span class="dt">Term</span></span>
<span id="cb7-2"><a href="#cb7-2"></a>term <span class="ot">=</span></span>
<span id="cb7-3"><a href="#cb7-3"></a>  parenthesizedTerm    <span class="co">-- (t)</span></span>
<span id="cb7-4"><a href="#cb7-4"></a>  <span class="op">&lt;|&gt;</span> letExpression    <span class="co">-- let x = t in t</span></span>
<span id="cb7-5"><a href="#cb7-5"></a>  <span class="op">&lt;|&gt;</span> caseExpression   <span class="co">-- case t of branches</span></span>
<span id="cb7-6"><a href="#cb7-6"></a>  <span class="op">&lt;|&gt;</span> lambdaExpression <span class="co">-- \x. t</span></span>
<span id="cb7-7"><a href="#cb7-7"></a>  <span class="op">&lt;|&gt;</span> forallExpression <span class="co">-- forall x. t</span></span>
<span id="cb7-8"><a href="#cb7-8"></a>  <span class="op">&lt;|&gt;</span> var              <span class="co">-- x</span></span></code></pre></div>
<p>These alternatives are tried in order, which means that to reach e.g. the <code>forall</code> case, the parser will try to parse the first token of each of the four preceding alternatives.</p>
<p>But note that the first character of each alternative rules out all other cases, save for (sometimes) the <code>var</code> case.</p>
<p>So the idea here is to rewrite the parser like this:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb8-1"><a href="#cb8-1"></a><span class="ot">term ::</span> <span class="dt">Parser</span> <span class="dt">Term</span></span>
<span id="cb8-2"><a href="#cb8-2"></a>term <span class="ot">=</span> <span class="kw">do</span></span>
<span id="cb8-3"><a href="#cb8-3"></a>  firstChar <span class="ot">&lt;-</span> lookAhead anyChar</span>
<span id="cb8-4"><a href="#cb8-4"></a>  <span class="kw">case</span> firstChar <span class="kw">of</span></span>
<span id="cb8-5"><a href="#cb8-5"></a>    <span class="ch">&#39;(&#39;</span> <span class="ot">-&gt;</span></span>
<span id="cb8-6"><a href="#cb8-6"></a>      parenthesizedTerm</span>
<span id="cb8-7"><a href="#cb8-7"></a></span>
<span id="cb8-8"><a href="#cb8-8"></a>    <span class="ch">&#39;l&#39;</span> <span class="ot">-&gt;</span></span>
<span id="cb8-9"><a href="#cb8-9"></a>      letExpression</span>
<span id="cb8-10"><a href="#cb8-10"></a>      <span class="op">&lt;|&gt;</span> var</span>
<span id="cb8-11"><a href="#cb8-11"></a></span>
<span id="cb8-12"><a href="#cb8-12"></a>    <span class="ch">&#39;c&#39;</span> <span class="ot">-&gt;</span></span>
<span id="cb8-13"><a href="#cb8-13"></a>      caseExpression</span>
<span id="cb8-14"><a href="#cb8-14"></a>      <span class="op">&lt;|&gt;</span> var</span>
<span id="cb8-15"><a href="#cb8-15"></a></span>
<span id="cb8-16"><a href="#cb8-16"></a>    <span class="ch">&#39;\\&#39;</span> <span class="ot">-&gt;</span></span>
<span id="cb8-17"><a href="#cb8-17"></a>      lambdaExpression</span>
<span id="cb8-18"><a href="#cb8-18"></a></span>
<span id="cb8-19"><a href="#cb8-19"></a>    <span class="ch">&#39;f&#39;</span> <span class="ot">-&gt;</span></span>
<span id="cb8-20"><a href="#cb8-20"></a>      forallExpression</span>
<span id="cb8-21"><a href="#cb8-21"></a>      <span class="op">&lt;|&gt;</span> var</span>
<span id="cb8-22"><a href="#cb8-22"></a></span>
<span id="cb8-23"><a href="#cb8-23"></a>    _ <span class="ot">-&gt;</span></span>
<span id="cb8-24"><a href="#cb8-24"></a>      var</span></code></pre></div>
<p>Now we just have to look at the first character to rule out the first four alternatives when parsing a <code>forall</code>.</p>
<p>Here are the results:</p>
<table>
<thead>
<tr class="header">
<th></th>
<th style="text-align: right;">Time</th>
<th style="text-align: right;">Delta</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Baseline</td>
<td style="text-align: right;">1.30 s</td>
<td style="text-align: right;"></td>
</tr>
<tr class="even">
<td>RTS flags</td>
<td style="text-align: right;">1.08 s</td>
<td style="text-align: right;">-17 %</td>
</tr>
<tr class="odd">
<td>Rock</td>
<td style="text-align: right;">0.613 s</td>
<td style="text-align: right;">-43 %</td>
</tr>
<tr class="even">
<td>Manual parallelisation</td>
<td style="text-align: right;">0.451 s</td>
<td style="text-align: right;">-26 %</td>
</tr>
<tr class="odd">
<td>Parser lookahead</td>
<td style="text-align: right;">0.442 s</td>
<td style="text-align: right;">-2 %</td>
</tr>
</tbody>
</table>
<p>Not great, but it's something.</p>
<h2 id="optimisation-5-dependent-hashmap">Optimisation 5: Dependent hashmap</h2>
<p>At this point, around 68 % of the time goes to operations on <a href="https://hackage.haskell.org/package/dependent-map"><code>Data.Dependent.Map</code></a>:</p>
<p><a href="../images/speeding-up-sixty/5-8ea6700415f1c46fb300571382ef438ae6082e8e.svg"><img src="../images/speeding-up-sixty/5-8ea6700415f1c46fb300571382ef438ae6082e8e.svg" /></a></p>
<p>Note that this was 15 % when we started out, so it has become the bottleneck only because we've fixed several others.</p>
<p><code>Data.Dependent.Map</code> implements a kind of dictionary data structure that allows the type of values to depend on the key, which is crucial for caching the result of queries, since each query may return a different type.</p>
<p><code>Data.Dependent.Map</code> is implemented as a clone of <code>Data.Map</code> from the <code>containers</code> package, adding this key-value dependency, so it's a binary tree that uses comparisons on the key type when doing insertions and lookups.</p>
<p>In the flamegraph above we can also see that around 21 % of the time goes to comparing the <code>Query</code> type. The reason for this slowness is likely that queries often contain strings, because most are things like "get the type of [name]". Strings are slow to compare because you need to traverse at least part of the string for each comparison.</p>
<p>It would be a better idea to use a hash map, because then the string usually only has to be traversed once, to compute the hash, but the problem is that there is no <em>dependent</em> hash map library in Haskell. Until now that is. I implemented a <a href="https://github.com/ollef/dependent-hashmap">dependent version of the standard <code>Data.HashMap</code></a> type from the <code>unordered-containers</code> as a thin wrapper around it. The results are as follows:</p>
<table>
<thead>
<tr class="header">
<th></th>
<th style="text-align: right;">Time</th>
<th style="text-align: right;">Delta</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Baseline</td>
<td style="text-align: right;">1.30 s</td>
<td style="text-align: right;"></td>
</tr>
<tr class="even">
<td>RTS flags</td>
<td style="text-align: right;">1.08 s</td>
<td style="text-align: right;">-17 %</td>
</tr>
<tr class="odd">
<td>Rock</td>
<td style="text-align: right;">0.613 s</td>
<td style="text-align: right;">-43 %</td>
</tr>
<tr class="even">
<td>Manual parallelisation</td>
<td style="text-align: right;">0.451 s</td>
<td style="text-align: right;">-26 %</td>
</tr>
<tr class="odd">
<td>Parser lookahead</td>
<td style="text-align: right;">0.442 s</td>
<td style="text-align: right;">-2 %</td>
</tr>
<tr class="even">
<td>Dependent hashmap</td>
<td style="text-align: right;">0.257 s</td>
<td style="text-align: right;">-42 %</td>
</tr>
</tbody>
</table>
<p>Having a look at the flamegraph after this change, we can see that <code>HashMap</code> operations take about 20 % of the total run time which is a lot better than 68 % (though there's still room for improvement). We can also see that the main bottleneck is now the parser:</p>
<p><a href="../images/speeding-up-sixty/6-722533c5d71871ca1aa6235fe79a53f33da99c36.svg"><img src="../images/speeding-up-sixty/6-722533c5d71871ca1aa6235fe79a53f33da99c36.svg" /></a></p>
<h2 id="optimisation-6-readert-based-rock-library">Optimisation 6: <code>ReaderT</code>-based Rock library</h2>
<p>Here's one that I did by ear, since it wasn't obvious from the profiling.</p>
<p>I mentioned that the Rock library used to support automatic parallelisation, but that I switched to doing it manually. A remnant from that is that the <code>Task</code> type in Rock is implemented in a needlessly inefficient way. <code>Task</code> is a monad that allows fetching queries, and is used throughout most of the Sixty compiler.</p>
<p>Before this change, <code>Task</code> was implemented roughly as follows:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb9-1"><a href="#cb9-1"></a><span class="kw">newtype</span> <span class="dt">Task</span> query a <span class="ot">=</span> <span class="dt">Task</span> {<span class="ot"> unTask ::</span> <span class="dt">IO</span> (<span class="dt">Result</span> query a) }</span>
<span id="cb9-2"><a href="#cb9-2"></a></span>
<span id="cb9-3"><a href="#cb9-3"></a><span class="kw">data</span> <span class="dt">Result</span> query a <span class="kw">where</span></span>
<span id="cb9-4"><a href="#cb9-4"></a>  <span class="dt">Done</span><span class="ot"> ::</span> a <span class="ot">-&gt;</span> <span class="dt">Result</span> query a</span>
<span id="cb9-5"><a href="#cb9-5"></a>  <span class="dt">Fetch</span><span class="ot"> ::</span> query a <span class="ot">-&gt;</span> (a <span class="ot">-&gt;</span> <span class="dt">Task</span> query b) <span class="ot">-&gt;</span> <span class="dt">Result</span> query b</span></code></pre></div>
<p>So to make a <code>Task</code> that fetches a query <code>q</code>, you need to create an <code>IO</code> action that returns a <code>Fetch q pure</code>. When doing automatic parallelisation, this allows introspecting whether a <code>Task</code> wants to do a fetch, such that independent fetches can be identified and run in parallel.</p>
<p>But actually, since we no longer support automatic parallelisation, this type might as well be implemented like this:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb10-1"><a href="#cb10-1"></a><span class="kw">newtype</span> <span class="dt">Task</span> query a <span class="ot">=</span> <span class="dt">Task</span> {<span class="ot"> unTask ::</span> <span class="dt">ReaderT</span> (<span class="dt">Fetch</span> query) <span class="dt">IO</span> a }</span>
<span id="cb10-2"><a href="#cb10-2"></a></span>
<span id="cb10-3"><a href="#cb10-3"></a><span class="kw">newtype</span> <span class="dt">Fetch</span> query <span class="ot">=</span> <span class="dt">Fetch</span> (<span class="kw">forall</span> a<span class="op">.</span> query a <span class="ot">-&gt;</span> <span class="dt">IO</span> a)</span></code></pre></div>
<p>The <code>ReaderT</code>-based implementation turns out to be a bit faster:</p>
<table>
<thead>
<tr class="header">
<th></th>
<th style="text-align: right;">Time</th>
<th style="text-align: right;">Delta</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Baseline</td>
<td style="text-align: right;">1.30 s</td>
<td style="text-align: right;"></td>
</tr>
<tr class="even">
<td>RTS flags</td>
<td style="text-align: right;">1.08 s</td>
<td style="text-align: right;">-17 %</td>
</tr>
<tr class="odd">
<td>Rock</td>
<td style="text-align: right;">0.613 s</td>
<td style="text-align: right;">-43 %</td>
</tr>
<tr class="even">
<td>Manual parallelisation</td>
<td style="text-align: right;">0.451 s</td>
<td style="text-align: right;">-26 %</td>
</tr>
<tr class="odd">
<td>Parser lookahead</td>
<td style="text-align: right;">0.442 s</td>
<td style="text-align: right;">-2 %</td>
</tr>
<tr class="even">
<td>Dependent hashmap</td>
<td style="text-align: right;">0.257 s</td>
<td style="text-align: right;">-42 %</td>
</tr>
<tr class="odd">
<td><code>ReaderT</code> in Rock</td>
<td style="text-align: right;">0.245 s</td>
<td style="text-align: right;">-5 %</td>
</tr>
</tbody>
</table>
<h2 id="optimisation-7-separate-lexer">Optimisation 7: Separate lexer</h2>
<p>Let's have a look at the flamegraph at this point in time: <a href="../images/speeding-up-sixty/7-048d2cec50e9994a0b159a2383580e3df5dd2a7e.svg"><img src="../images/speeding-up-sixty/7-048d2cec50e9994a0b159a2383580e3df5dd2a7e.svg" /></a></p>
<p>The parser now takes almost 30 % of the total run time. The parser is written using parser combinators that work directly on characters, so it's also doing tokenisation, or lexing, on the fly.</p>
<p>I've been wondering about the performance impact of this practice, since it's quite common in the Haskell world. So the change I made <a href="https://github.com/ollef/sixty/commit/11c46c5b03f26a66347d5f387bd4cdfd5f6de4a2">here</a> is to write a faster lexer that's separate from the parser, and then make the parser work on the list of tokens that the lexer spits out.</p>
<p>This turns out to be a great idea:</p>
<table>
<thead>
<tr class="header">
<th></th>
<th style="text-align: right;">Time</th>
<th style="text-align: right;">Delta</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Baseline</td>
<td style="text-align: right;">1.30 s</td>
<td style="text-align: right;"></td>
</tr>
<tr class="even">
<td>RTS flags</td>
<td style="text-align: right;">1.08 s</td>
<td style="text-align: right;">-17 %</td>
</tr>
<tr class="odd">
<td>Rock</td>
<td style="text-align: right;">0.613 s</td>
<td style="text-align: right;">-43 %</td>
</tr>
<tr class="even">
<td>Manual parallelisation</td>
<td style="text-align: right;">0.451 s</td>
<td style="text-align: right;">-26 %</td>
</tr>
<tr class="odd">
<td>Parser lookahead</td>
<td style="text-align: right;">0.442 s</td>
<td style="text-align: right;">-2 %</td>
</tr>
<tr class="even">
<td>Dependent hashmap</td>
<td style="text-align: right;">0.257 s</td>
<td style="text-align: right;">-42 %</td>
</tr>
<tr class="odd">
<td><code>ReaderT</code> in Rock</td>
<td style="text-align: right;">0.245 s</td>
<td style="text-align: right;">-5 %</td>
</tr>
<tr class="even">
<td>Separate lexer</td>
<td style="text-align: right;">0.154 s</td>
<td style="text-align: right;">-37 %</td>
</tr>
</tbody>
</table>
<p>The "inner loop" of the parser that I tried optimising in the "Parser lookahead" step has now become a case expression on the next token, visible <a href="https://github.com/ollef/sixty/blob/11c46c5b03f26a66347d5f387bd4cdfd5f6de4a2/src/Parser.hs#L543-L581">here</a>. Essentially, it's gone from matching on characters to this:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb11-1"><a href="#cb11-1"></a><span class="ot">term ::</span> <span class="dt">Parser</span> <span class="dt">Term</span></span>
<span id="cb11-2"><a href="#cb11-2"></a>term <span class="ot">=</span> <span class="kw">do</span></span>
<span id="cb11-3"><a href="#cb11-3"></a>  token <span class="ot">&lt;-</span> getNextToken</span>
<span id="cb11-4"><a href="#cb11-4"></a>  <span class="kw">case</span> token <span class="kw">of</span></span>
<span id="cb11-5"><a href="#cb11-5"></a>    <span class="dt">Lexer.LeftParen</span> <span class="ot">-&gt;</span> parenthesizedTerm</span>
<span id="cb11-6"><a href="#cb11-6"></a>    <span class="dt">Lexer.Let</span> <span class="ot">-&gt;</span> letExpression</span>
<span id="cb11-7"><a href="#cb11-7"></a>    <span class="dt">Lexer.Identifier</span> ident <span class="ot">-&gt;</span> <span class="fu">pure</span> (<span class="dt">Var</span> ident)</span>
<span id="cb11-8"><a href="#cb11-8"></a>    <span class="dt">Lexer.Case</span> <span class="ot">-&gt;</span> caseExpression</span>
<span id="cb11-9"><a href="#cb11-9"></a>    <span class="dt">Lexer.Lambda</span> <span class="ot">-&gt;</span> lambdaExpression</span>
<span id="cb11-10"><a href="#cb11-10"></a>    <span class="dt">Lexer.Forall</span> <span class="ot">-&gt;</span> forallExpression</span>
<span id="cb11-11"><a href="#cb11-11"></a>    <span class="dt">Lexer.Number</span> int <span class="ot">-&gt;</span> <span class="fu">pure</span> (<span class="dt">Lit</span> int)</span></code></pre></div>
<h2 id="optimisation-8-faster-hashing">Optimisation 8: Faster hashing</h2>
<p>The flamegraph at this point contains mostly things I don't really know what to do with, but there's one thing left, and that's hashing of queries, which now takes just short of 18 % of the total runtime: <a href="../images/speeding-up-sixty/8-11c46c5b03f26a66347d5f387bd4cdfd5f6de4a2.svg"><img src="../images/speeding-up-sixty/8-11c46c5b03f26a66347d5f387bd4cdfd5f6de4a2.svg" /></a></p>
<p>The change I made <a href="https://github.com/ollef/sixty/commit/d5bad6f606450d0a2c8926072e7b4845d982b81f">here</a> is to write some <code>Hashable</code> instances by hand instead of deriving them, and to add couple of inlining pragmas. This gives a 5 % speedup:</p>
<table>
<thead>
<tr class="header">
<th></th>
<th style="text-align: right;">Time</th>
<th style="text-align: right;">Delta</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Baseline</td>
<td style="text-align: right;">1.30 s</td>
<td style="text-align: right;"></td>
</tr>
<tr class="even">
<td>RTS flags</td>
<td style="text-align: right;">1.08 s</td>
<td style="text-align: right;">-17 %</td>
</tr>
<tr class="odd">
<td>Rock</td>
<td style="text-align: right;">0.613 s</td>
<td style="text-align: right;">-43 %</td>
</tr>
<tr class="even">
<td>Manual parallelisation</td>
<td style="text-align: right;">0.451 s</td>
<td style="text-align: right;">-26 %</td>
</tr>
<tr class="odd">
<td>Parser lookahead</td>
<td style="text-align: right;">0.442 s</td>
<td style="text-align: right;">-2 %</td>
</tr>
<tr class="even">
<td>Dependent hashmap</td>
<td style="text-align: right;">0.257 s</td>
<td style="text-align: right;">-42 %</td>
</tr>
<tr class="odd">
<td><code>ReaderT</code> in Rock</td>
<td style="text-align: right;">0.245 s</td>
<td style="text-align: right;">-5 %</td>
</tr>
<tr class="even">
<td>Separate lexer</td>
<td style="text-align: right;">0.154 s</td>
<td style="text-align: right;">-37 %</td>
</tr>
<tr class="odd">
<td>Faster hashing</td>
<td style="text-align: right;">0.146 s</td>
<td style="text-align: right;">-5 %</td>
</tr>
</tbody>
</table>
<p>The new flamegraph shows that query hashing is now down to around 11 % of the time. <a href="../images/speeding-up-sixty/9-d5bad6f606450d0a2c8926072e7b4845d982b81f.svg"><img src="../images/speeding-up-sixty/9-d5bad6f606450d0a2c8926072e7b4845d982b81f.svg" /></a></p>
<h2 id="conclusion">Conclusion</h2>
<p>I was able to make the Sixty compiler nine times faster for this benchmark by using the excellent profiling tools that we have for Haskell. There's no reason to be optimising in the dark here.</p>
<p>As a reminder, here's what the compiler looked like in ThreadScope to start with:</p>
<p><a href="../images/speeding-up-sixty/0-29094e006d4c88f51d744b0fd26f3e2e18af3ce0-threadscope.png"><img src="../images/speeding-up-sixty/0-29094e006d4c88f51d744b0fd26f3e2e18af3ce0-threadscope.png" /></a></p>
<p>Here's where we're at now:</p>
<p><a href="../images/speeding-up-sixty/9-d5bad6f606450d0a2c8926072e7b4845d982b81f-threadscope.png"><img src="../images/speeding-up-sixty/9-d5bad6f606450d0a2c8926072e7b4845d982b81f-threadscope.png" /></a></p>
<p>It looks faster and it <em>is</em> faster.</p>
      ]]></content>
      </entry>
</feed>
